{"cells":[{"cell_type":"markdown","source":["### **TFM - Label reduction to 95% of Dataset. Reduction from 7201 to 453 labels. (13/01/2025)**"],"metadata":{"id":"hUoGAD-Vv_Nt"}},{"cell_type":"markdown","source":["* See\n","https://discuss.huggingface.co/t/most-efficient-multi-label-classifier/9296/2\n","\n","* The Artificial Guy - MULTI-LABEL TEXT CLASSIFICATION USING BERT AND PYTORCH\n","https://www.youtube.com/watch?v=f-86-HcYYi8\n","\n","* Saurabh Anand - BERT for Multi-Label Classification\n","https://www.youtube.com/watch?v=JjcxZPNZbUY\n","\n","* KGP Talkie - 5 - Multi-Label Text Classification Model with DistilBERT and Hugging Face Transformers in PyTorch\n","https://www.youtube.com/watch?v=ZYc9za75Chk\n","\n","* Fine Tuning BERT for a Multi-Label Classification Problem on Colab - https://medium.com/@abdurhmanfayad_73788/fine-tuning-bert-for-a-multi-label-classification-problem-on-colab-5ca5b8759f3f\n","\n","* BERT and DistilBERT Models for NLP - https://medium.com/@kumari01priyanka/bert-and-distilbert-model-for-nlp-7352eb16915e\n","\n","* Choosing the Right Colab Runtime: A Guide for Data Scientists and Analysts - https://drlee.io/choosing-the-right-colab-runtime-a-guide-for-data-scientists-and-analysts-57ee7b7c9638\n","\n","* distilbert / distilbert-base-uncased - https://huggingface.co/distilbert/distilbert-base-uncased\n","\n","* \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\" - https://arxiv.org/abs/1910.01108"],"metadata":{"id":"KkFdEWhYdBSu"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"PyHD3zLiOqwk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738964533184,"user_tz":-60,"elapsed":20308,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"c5c73e25-9c20-45a0-ab80-650ce3aff624"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/TFM-MUECIM/*.py /content\n","!cp /content/drive/MyDrive/TFM-MUECIM/*.txt /content\n","!cp /content/drive/MyDrive/TFM-MUECIM/*.dat /content\n","!cp /content/drive/MyDrive/TFM-MUECIM/*.pt /content\n","!cp /content/drive/MyDrive/TFM-MUECIM/*.csv /content\n","!cp /content/drive/MyDrive/TFM-MUECIM/*.tar /content\n","!cd /content; tar xf data.tar data\n","!cd /content/drive/MyDrive/TFM-MUECIM"],"metadata":{"id":"NSTcMCSL-iyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8xMexEYKRZe","executionInfo":{"status":"ok","timestamp":1738779371009,"user_tz":-60,"elapsed":2539,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"aad9084c-2649-4bee-e2a5-3642408614d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"]}]},{"cell_type":"code","source":["import sys\n","baseDir = '/content' #/drive/My Drive/TFM-MUECIM'\n","sys.path.append(baseDir)"],"metadata":{"id":"NB3DgnyvR_-5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Segons el notebook:\n","Fine-tuning BERT (and friends) for multi-label text classification.ipynb\n","\n","https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=HgpKXDfvKBxn  "],"metadata":{"id":"qlpU-ZdpS8ju"}},{"cell_type":"code","source":["import torch\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","# Move the model to the correct device before training.\n","\n","# ensures reproducibility\n","torch.manual_seed(0)"],"metadata":{"id":"kgtNCkrOTj6s","executionInfo":{"status":"ok","timestamp":1738779386561,"user_tz":-60,"elapsed":5134,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"4439b319-6d17-4056-e77c-f559c17dab13","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7a6afcb72a70>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from tfm_EURLEX57KDataset import EURLEX57KDataset\n","from torch.utils.data import random_split\n","\n","ds = EURLEX57KDataset(baseDir,'ReducedEURLEX57KDataFrame.csv')\n","fullSetSize = ds.__len__()\n","trainSetSize = int(fullSetSize * 0.8)\n","valSetSize = int(fullSetSize * 0.1)\n","testSetSize = fullSetSize - trainSetSize - valSetSize\n","print(f'Full set size: {fullSetSize}')\n","print(f'Train set size: {trainSetSize}')\n","print(f'Validation set size: {valSetSize}')\n","print(f'Test set size: {testSetSize}')\n","trainData, valData, testData = random_split(\n","    ds,\n","    [trainSetSize, valSetSize, testSetSize]\n",")"],"metadata":{"id":"YI2J80j5dUxt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737915707193,"user_tz":-60,"elapsed":2888,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"9606219e-6d56-4253-d031-0a44c1ee8e22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Full set size: 54382\n","Train set size: 43505\n","Validation set size: 5438\n","Test set size: 5439\n"]}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# set batch size\n","batchSize = 10\n","\n","# create dataloaders. In case we'll use a more classical pipeline approach\n","trainDataLoader = DataLoader(trainData, batch_size=batchSize, shuffle=True)\n","valDataLoader = DataLoader(valData, batch_size=batchSize, shuffle=True)\n","testDataLoader = DataLoader(testData, batch_size=batchSize, shuffle=True)"],"metadata":{"id":"elAZ80QQmyux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def countNonZeroItems(items):\n","    nonZero = torch.nonzero(items, as_tuple= True)\n","    return len(nonZero[0])"],"metadata":{"id":"wHYp827HSXSF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# iterate through val batches\n","for i, batch in enumerate(valDataLoader):\n","  print(f'Batch {i}: ')\n","  batchFileNames = batch.get('fileName')\n","  batchData = batch.get('input_ids')\n","  batchAttentionMasks = batch.get('attention_mask')\n","  batchLabels = batch.get('labels')\n","\n","  for elem in zip(batchFileNames, batchData, batchAttentionMasks, batchLabels):\n","    print(f'fileName: {elem[0]}')\n","    print(f'input_ids (5 first elements):\\n{elem[1][0:5]}')\n","    print(f'attention_masks (5 first elements):\\n{elem[2][0:5]}')\n","    print(f'Nonzero labels:{countNonZeroItems(elem[3])}\\n')\n","\n","  break\n","\n","print('Done!')"],"metadata":{"id":"X78654y-mkRt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737915707194,"user_tz":-60,"elapsed":10,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"c175a3ed-a79a-4746-f0f7-db0d5ead2380"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 0: \n","fileName: data/datasets/EURLEX57K/train/32010R0503.json\n","input_ids (5 first elements):\n","tensor([3222, 7816, 1006, 7327, 1007])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:6\n","\n","fileName: data/datasets/EURLEX57K/train/32005R1314.json\n","input_ids (5 first elements):\n","tensor([ 3222,  7816,  1006, 14925,  1007])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:3\n","\n","fileName: data/datasets/EURLEX57K/train/32013R0735.json\n","input_ids (5 first elements):\n","tensor([ 2473, 14972,  7816,  1006,  7327])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:1\n","\n","fileName: data/datasets/EURLEX57K/train/32001R1485.json\n","input_ids (5 first elements):\n","tensor([ 7816,  1006, 14925,  1007,  2053])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:2\n","\n","fileName: data/datasets/EURLEX57K/train/32007L0052.json\n","input_ids (5 first elements):\n","tensor([ 3222, 16449,  2289,  1013,  4720])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:2\n","\n","fileName: data/datasets/EURLEX57K/test/32007R1389.json\n","input_ids (5 first elements):\n","tensor([ 3222,  7816,  1006, 14925,  1007])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:1\n","\n","fileName: data/datasets/EURLEX57K/train/31985R0143.json\n","input_ids (5 first elements):\n","tensor([ 3222,  7816,  1006, 25212,  2278])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:3\n","\n","fileName: data/datasets/EURLEX57K/train/31995R1202.json\n","input_ids (5 first elements):\n","tensor([ 3222,  7816,  1006, 14925,  1007])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:4\n","\n","fileName: data/datasets/EURLEX57K/test/31997R2526.json\n","input_ids (5 first elements):\n","tensor([ 3222,  7816,  1006, 14925,  1007])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:2\n","\n","fileName: data/datasets/EURLEX57K/train/32014D0470.json\n","input_ids (5 first elements):\n","tensor([ 2297,  1013, 21064,  1013,  7327])\n","attention_masks (5 first elements):\n","tensor([1, 1, 1, 1, 1])\n","Nonzero labels:1\n","\n","Done!\n"]}]},{"cell_type":"code","source":["# bert huggingface pretrained model\n","import os\n","from transformers import AutoConfig\n","from transformers import DistilBertForSequenceClassification\n","from tfm_ReducedLabelIndex import LabelIndex\n","\n","labelIndex = LabelIndex(baseDir)\n","\n","# Gemini. Define a cache directory for Hugging Face models and ensure it exists.\n","cache_dir = os.path.join(baseDir, 'tfm_cache')\n","os.makedirs(cache_dir, exist_ok=True)\n","\n","# Load the configuration with the cache directory.\n","config = AutoConfig.from_pretrained(\n","    'distilbert-base-uncased',\n","    force_download=True,\n","    cache_dir=cache_dir,\n","    num_labels=labelIndex.numLabels,\n","    problem_type='multi_label_classification',\n","    id2label=labelIndex.id2label,\n","    label2id=labelIndex.label2id\n",")\n"],"metadata":{"id":"W1-0V35mnfgT","colab":{"base_uri":"https://localhost:8080/","height":178,"referenced_widgets":["c97a3f838a774173bf2cbec2f7470079","8fd921080f764ff9b639ec95eb122f2a","d33d27dd77484516b3e6293ff3b92642","424fca16d9ca4c7b9d4f6d083bb48bcd","375b9192b13d481489ad7af9048d2ca6","311bb02800d0411ab37ade97a6786c89","ce033def281645f79df4b0200fb1afbd","6d163222e80243a5a5885f056d44e2a2","71bae1fae88a42f8a4b3d48020fbef46","75d741ebcc254a22b979ccbb8e722e03","37cd2ca1daa34252a4feaa9ffc72a705"]},"executionInfo":{"status":"ok","timestamp":1737915713278,"user_tz":-60,"elapsed":6092,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"50b461d2-a494-44ca-a431-e2560415b1a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97a3f838a774173bf2cbec2f7470079"}},"metadata":{}}]},{"cell_type":"code","source":["labelIndex.numLabels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYd7-d_j7lsS","executionInfo":{"status":"ok","timestamp":1737915713279,"user_tz":-60,"elapsed":14,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"2c96b431-f7cd-4b1f-f892-b2d06ff48969"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["453"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# 0 means pretrained fresh model\n","# 1, 2, 3 ... n. trained epochs from file\n","last_epoch_trained = 18\n","\n","if last_epoch_trained == 1:\n","  modelFile = '20250122_tfm_model.pt'\n","if last_epoch_trained == 2:\n","  modelFile = '20250124_tfm_model_1.pt'\n","if last_epoch_trained == 3:\n","  modelFile = '20250124_tfm_model_2.pt'\n","if last_epoch_trained == 6:\n","  modelFile = '20250124_tfm_model_3.pt'\n","if last_epoch_trained == 9:\n","  modelFile = '20250126_tfm_model_1.pt'\n","if last_epoch_trained == 12:\n","  modelFile = '20250126_tfm_model_2.pt'\n","if last_epoch_trained == 15:\n","  modelFile = '20250126_tfm_model_3.pt'\n","if last_epoch_trained == 18:\n","  modelFile = '20250126_tfm_model_4.pt'"],"metadata":{"id":"OrovENWid0fj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the model with the configuration and cache directory.\n","if last_epoch_trained == 0:\n","  model = DistilBertForSequenceClassification.from_pretrained(\n","    'distilbert-base-uncased', # Changed to the correct model identifier\n","    config=config,  # Pass the configuration to the model.\n","    cache_dir=cache_dir  # Specify the cache directory again.\n","  )\n","else:\n","  model = torch.load(os.path.join(baseDir,modelFile), map_location=torch.device(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNuldwVEdl1B","executionInfo":{"status":"ok","timestamp":1737915713669,"user_tz":-60,"elapsed":399,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"29722da5-e5d3-487e-b74e-219a81ad9da3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-13-dd5254b7b697>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(os.path.join(baseDir,modelFile), map_location=torch.device(device))\n"]}]},{"cell_type":"code","source":["model"],"metadata":{"id":"XpP9kR-7bA6i","executionInfo":{"status":"ok","timestamp":1737915713669,"user_tz":-60,"elapsed":15,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad5e9b82-cbc2-46c2-aab2-4526c8411c73"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): DistilBertSdpaAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=453, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["\n","# forward pass. no training. test case\n","# item = trainData.__getitem__(0)\n","\n","# outputs = model(\n","#    input_ids=item['input_ids'][0:512].unsqueeze(0),\n","#    attention_mask=item['attention_mask'][0:512].unsqueeze(0),\n","#    labels=item['labels'].unsqueeze(0))"],"metadata":{"id":"PxmV0SKKz0Xm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# outputs.logits[0]\n"],"metadata":{"id":"GHvBt-L5rxFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n","# calculate metrics\n","# import numpy as np\n","# from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n","\n","# sigmoid = torch.nn.Sigmoid()\n","# probs = sigmoid(outputs.logits[0])\n","# threshold = 0.5\n","# y_pred = np.zeros(probs.shape)\n","# y_pred[np.where(probs >= threshold)] = 1\n","# y_true = item['labels'].cpu().numpy() # Convert y_true to a NumPy array\n","# f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n","# roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n","# accuracy = accuracy_score(y_true, y_pred)\n","\n","# metrics = {'f1': f1_micro_average,\n","#           'roc_auc': roc_auc,\n","#           'accuracy': accuracy}\n"],"metadata":{"id":"YfpY8psYMQDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# metrics  # code test"],"metadata":{"id":"Pbpm8ZLqN8y6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\n","# https://towardsdatascience.com/evaluating-multi-label-classifiers-a31be83da6ea\n","\n","import numpy as np\n","from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n","import torch\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers import EvalPrediction\n","\n","\n","def multi_label_metrics(predictions, labels, ):\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(torch.Tensor(predictions))\n","    y_pred = np.zeros(probs.shape)\n","    y_true = labels\n","    y_pred[np.where(probs >= 0.5)] = 1\n","    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n","    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n","    accuracy = accuracy_score(y_true, y_pred)\n","    # define dictionary of metrics to return\n","    metrics = {'f1': f1_micro_average,\n","               'roc_auc': roc_auc,\n","               'accuracy': accuracy}\n","    return metrics\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions,\n","            tuple) else p.predictions\n","    result = multi_label_metrics(\n","        predictions=preds,\n","        labels=p.label_ids)\n","    return result\n","\n","def metricsForTestSet():\n","    predictions = trainer.predict(testData)\n","    preds = predictions.predictions[0] if isinstance(predictions.predictions, tuple) else predictions.predictions\n","    labels = predictions.label_ids\n","    testMetrics = multi_label_metrics(predictions=preds, labels=labels)\n","    print(testMetrics)\n","\n","# metric\n","metricName = 'f1'\n","\n","# training arguments\n","trainArgs = TrainingArguments(\n","    'tfm_oputput',\n","    report_to='none',  # deactivate wandb  reports. Alternative -> TensorBoard\n","    evaluation_strategy = 'epoch',\n","    save_strategy = 'epoch',\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batchSize,\n","    per_device_eval_batch_size=batchSize,\n","    num_train_epochs=3, # 3 epochs\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=metricName)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=trainArgs,\n","    train_dataset=trainData,\n","    eval_dataset=valData,\n","    compute_metrics = compute_metrics,\n","    #data_collator = Data_Processing(),\n",")\n","\n","# API-KEY-WAND-LIBRARY: 1bb618394e7e44feab7f79534fa2be428243d1bb\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","# Move the model to the correct device before training.\n","model.to(device)\n","\n","# epoch 0 - baseline\n","print('Begin epoch train session - trainer evaluate')\n","trainer.evaluate()\n","\n","print('Metrics for test set (before train)')\n","metricsForTestSet()\n","\n","# training\n","print('Epoch train.')\n","trainer.train()\n","print('Epoch train done.')\n","\n","print('End epoch train session - trainer evaluate')\n","trainer.evaluate()\n","\n","print('Metrics for test set (after train)')\n","metricsForTestSet()\n","\n","print('End epoch train session')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"um7ZpE1y5xOi","outputId":"b0e12620-3b71-427d-e982-3ff50eeebe3e","executionInfo":{"status":"ok","timestamp":1737917267162,"user_tz":-60,"elapsed":1105307,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Begin epoch train session - trainer evaluate\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Metrics for test set (before train)\n","{'f1': 0.7846710290179187, 'roc_auc': 0.8739954214164557, 'accuracy': 0.36992094134951276}\n","Epoch train.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3775' max='13053' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 3775/13053 06:41 < 16:27, 9.39 it/s, Epoch 0.87/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13053' max='13053' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13053/13053 24:22, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Model Preparation Time</th>\n","      <th>F1</th>\n","      <th>Roc Auc</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.002700</td>\n","      <td>0.014422</td>\n","      <td>0.001500</td>\n","      <td>0.773243</td>\n","      <td>0.862745</td>\n","      <td>0.333946</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.002700</td>\n","      <td>0.014209</td>\n","      <td>0.001500</td>\n","      <td>0.778159</td>\n","      <td>0.869101</td>\n","      <td>0.348106</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.005500</td>\n","      <td>0.013577</td>\n","      <td>0.001500</td>\n","      <td>0.783571</td>\n","      <td>0.875154</td>\n","      <td>0.358220</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch train done.\n","End epoch train session - trainer evaluate\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Metrics for test set (after train)\n","{'f1': 0.7842010182295943, 'roc_auc': 0.876986260975051, 'accuracy': 0.3669792241220813}\n","End epoch train session\n"]}]},{"cell_type":"code","source":["# https://stackoverflow.com/questions/42703500/how-do-i-save-a-trained-model-in-pytorch\n","import shutil\n","from datetime import datetime\n","\n","prefixDate = datetime.today().strftime('%Y%m%d')\n","fileName = f'{prefixDate}_tfm_model.pt'\n","modelFullPath = os.path.join(baseDir,fileName)\n","drivePath = '/content/drive/MyDrive/TFM-MUECIM'\n","destFullPath = os.path.join(drivePath,fileName)\n","print('Save model after epoch train session')\n","torch.save(model, modelFullPath)\n","shutil.copyfile(modelFullPath, destFullPath)\n","\n"],"metadata":{"id":"MGzXQk8FU7bp","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1737917268286,"user_tz":-60,"elapsed":1125,"user":{"displayName":"Albert Baranguer","userId":"04291152240601656998"}},"outputId":"ed208f89-f4e9-4238-9cfa-f1f38bac5a59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Save model after epoch train session\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/TFM-MUECIM/20250126_tfm_model.pt'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]}],"metadata":{"colab":{"provenance":[{"file_id":"1ewwKVm9FPvrB9GaRPsXZ35y_xjFbwZRP","timestamp":1736724742726},{"file_id":"/v2/external/notebooks/pro.ipynb","timestamp":1735915359838}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c97a3f838a774173bf2cbec2f7470079":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8fd921080f764ff9b639ec95eb122f2a","IPY_MODEL_d33d27dd77484516b3e6293ff3b92642","IPY_MODEL_424fca16d9ca4c7b9d4f6d083bb48bcd"],"layout":"IPY_MODEL_375b9192b13d481489ad7af9048d2ca6"}},"8fd921080f764ff9b639ec95eb122f2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_311bb02800d0411ab37ade97a6786c89","placeholder":"​","style":"IPY_MODEL_ce033def281645f79df4b0200fb1afbd","value":"config.json: 100%"}},"d33d27dd77484516b3e6293ff3b92642":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d163222e80243a5a5885f056d44e2a2","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71bae1fae88a42f8a4b3d48020fbef46","value":483}},"424fca16d9ca4c7b9d4f6d083bb48bcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75d741ebcc254a22b979ccbb8e722e03","placeholder":"​","style":"IPY_MODEL_37cd2ca1daa34252a4feaa9ffc72a705","value":" 483/483 [00:00&lt;00:00, 37.6kB/s]"}},"375b9192b13d481489ad7af9048d2ca6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"311bb02800d0411ab37ade97a6786c89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce033def281645f79df4b0200fb1afbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d163222e80243a5a5885f056d44e2a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71bae1fae88a42f8a4b3d48020fbef46":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75d741ebcc254a22b979ccbb8e722e03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37cd2ca1daa34252a4feaa9ffc72a705":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}